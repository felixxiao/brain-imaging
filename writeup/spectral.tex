% Need to fix optimization problems -- add spacing between min and obj
%#######################################################################

In the previous chapter, we showed how local search heuristics produced
parcels that were balanced and had high within-parcel and low
between-parcel edge weights. The central idea behind such methods
was to choose vertices to be in the same component if the edge
connecting them has high distance correlation. Vertices were added to
components one-by-one with constraints on component size, but not on
component shape. As a result, one salient issue with these
parcellations was lack of smoothness, or regularity in the parcels'
spatial shapes. There was scant resemblence between the anatomical maps
of the brain depicting smooth, rotund lobes and our jagged, web-like
parcellations.

One key reason for this phenomenon are the local search heuristics'
focus on maximizing \textit{average} within-component edge weights
(equivalently, minimizing \textit{average} between-component edge
weights because edges are either within the same component or between
different components). To get smoothness in the boundary between
components, we could either impose a penalty for too many
between-component edges and work that into the local search heuristics,
or try minimizing over the sum of all weights on between-component
edges. This chapter deals with the second approach and this family of
methods is called spectral partitioning.

Spectral partitioning constitutes the second major class of techniques
used to partition graphs. Rather than rely on local component-growing
heuristics, spectral partitioning uses information about the entire
graph at once.

Throughout this chapter, a valid partitioning
$P_k = (V_1, ..., V_k)$ of the graph $G = (V, E)$ is defined in the
same way as in chapter 3; i.e., it must satisfy

\begin{enumerate}[1.]
\item
$V_i \neq \emptyset$ for all $V_i \in \mathcal{P}_k$

\item
$\bigcup\limits_{i=1}^k V_i = V$

\item
$V_i \cap V_j = \emptyset$ for all $V_i, V_j \in \mathcal{P}_k$

\item
$V_i$ is connected (i.e. for every two vertices in $V_i$, there is a
path between them) for all $V_i \in \mathcal{P}_k$
\end{enumerate}

For all edges $(i,j) \in E$, let $w_{ij}$ denote the weight of the edge
connecting vertices $i$ and $j$.
$S^{n \times n}$ is the set of real symmetric $n \times n$ matrices.
We further define, for a given graph $G = (V, E)$, the associated

\begin{definition}
Adjacency matrix. $A \in \mathcal{S}^{n \times n}$ has entries
\[
A_{ij} = \begin{cases}
  w_{ij} & \text{if } (i,j) \in E \\
  0      & \text{otherwise} \\
\end{cases}
\]
\end{definition}

\begin{definition}
Degree matrix. $D \in \mathcal{S}^{n \times n}$
\[
D_{ij} = \begin{cases}
  \sum_{k = 1}^n A_{ik} & \text{if } i = j \\
  0                     & \text{otherwise} \\
\end{cases}
\]
\end{definition}

\section{Size-Constrained MinCut and Graph Bipartitioning}

Consider the case $k = 2$. For all $i \in V$, let $x_i = 1$ if
$i \in V_1$ and $x_1 = -1$ if $i \in V_2$. Then the sum of weights on
edges between the two components is
\begin{align*}
C(P_2)
&= \sum_{i \in V_1} \sum_{j \in V_2} A_{ij} \\
&= \sum_{i = 2}^n \sum_{j = 1}^{i-1} \frac{(x_i - x_j)^2}{4} A_{ij} 
\end{align*}
since
\[ (x_i - x_j)^2 = \begin{cases}
	4 & \mbox{if } i,j \mbox{ are in different components} \\
	0 & \mbox{otherwise}
\end{cases}\]

$C(P_2)$ can also be written in a matrix quadratic form, as

\begin{align*}
C(P_2)
&= \sum_{i = 2}^n \sum_{j = 1}^{i-1} \frac{(x_i - x_j)^2}{4} A_{ij} \\
&= \frac{1}{2} \sum_{i,j = 1}^n \frac{(x_i - x_j)^2}{4} A_{ij} \\
&= \frac{1}{2} \sum_{i,j = 1}^n
   \frac{x_i^2 + x_j^2 - 2 x_i x_j}{4} A_{ij} \\
&= \frac{1}{2} \sum_{i,j = 1}^n \frac{1 - x_i x_j}{2} A_{ij} \\
&= \frac{1}{4} \sum_{i,j = 1}^n (x_i^2 - x_i x_j) A_{ij} \\
&= \frac{1}{4} \sum_{i = 1}^n x_i^2 \sum_{j = 1}^n A_{ij}
 - \frac{1}{4} \sum_{i,j = 1}^n x_i A_{ij} x_j \\
&= \frac{1}{4} \sum_{i = 1}^n x_i^2 D_{ii} - \frac{1}{4} x^T A x \\
&= \frac{1}{4} x^T (D - A) x \\
&= \frac{1}{4} x^T L x
\end{align*}
where $L$ is called the Laplacian matrix of the graph and defined as
$L = D - A$. MinCut can thus be formulated as minimizing $x^T L x$
subject to $x \in \{-1, 1\}^n$.

Algorithms like Karger's can solve MinCut in polynomial time. However,
MinCut in this formulation lacks constraints on the size of the
partitions, and if applied to our brain parcellation problem, would
result in severely inbalanced partitions. If constraints on the sizes
of the components were added, the problem becomes NP-hard [citation].

An old but effective approach to bipartitioning uses the eigenvectors
of the Laplacian matrix and is called spectral bipartitioning.
The approach relaxes the $\{-1, 1\}$ constraint on $x$ (and rescales
$x$) so that it need only satisfy $\|x\| = 1$ ($\|\cdot\|$ here refering
to L2 norm). It is easy to see that
$\big\{ x : x \in \{-\frac{1}{\sqrt{n}}, \frac{1}{\sqrt{n}}\}^n \big\}
 \subset \big\{ x \in \R^n : \|x\| = 1 \big\}$
The problem now becomes

\begin{equation} \label{spectral_bipartition}
\begin{aligned}
\min_x      &\;& x^T L x \\
\text{s.t.} &\;& \| x \| = 1 \\
\end{aligned}
\end{equation}

Using Lagrangian multipliers, it can be shown that all optimal solutions
to the above must satisfy $L x = \lambda x$ and this problem reduces to
finding the smallest eigenvalues of $L$ and their associated
eigenvectors. In addition, \ref{Laplacian_psd} below implies that all
eigenvalues are nonnegative.

\begin{theorem} \label{Laplacian_psd}
Let $L$ be a Laplacian matrix. Then $L \succeq 0$
($L$ is positive semidefinite)

Proof. Let $x \in \R^n$. $x^T L x = x^T D x - $
\end{theorem}

Note that from the
$C(P_2) = \sum_{i > j} \frac{(x_i - x_j)^2}{4} A_{ij}
        = \frac{1}{4} x^T L x$ equivalence we know that
$0$ and $(\frac{1}{\sqrt{n}}, ..., \frac{1}{\sqrt{n}})^T$ is a minimum
eigenvalue and eigenvector to this system. For bipartitioning, the
useful eigenvector is the one that corresponds to the 2nd smallest
eigenvalue, which is nonzero if the graph as a whole is connected.
We'll denote this eigenvalue as $\lambda_1$ and corresponding unit
eigenvector as $x_1$. We have the following:

\begin{theorem}
Let $P_2$ be any valid partition into 2 components. Then
$C(P_2) \geq \lambda_1$

Proof.
\end{theorem}

In the literature, $x_1$ is often refered to as the Fiedler vector,
after the first mathematician who studied it in detail [Fiedler 1975].
From the Fiedler vector we can obtain a variety of "good" bipartitions.
We can impose a size constraint $|V_1| = s$ and obtain a bipartition
satisfying this by placing the vertices associated with the $s$ largest
entries of $x_1$ in $V_1$. This encompasses bipartitions of equal
component size. We can also sort the entries of $x_1$ and find the
largest difference between consecutive sorted entries. Vertices
corresponding to entries sorted to the left of this split can be placed
in $V_1$ and vertices sorted to the right in $V_2$. This method tends to
approximate the MinCut solution.

The result of spectral bipartitioning on a resting state fMRI scan
is shown below. As anticipated, the boundaries of between the
components are smooth.

\begin{center}
\includegraphics[scale = 0.5]{5_spectral_2_axial.png}

Axial

\includegraphics[scale = 0.5]{5_spectral_2_coronal.png}

Coronal

\includegraphics[scale = 0.5]{5_spectral_2_sagittal.png}

Sagittal
\end{center}

One can recursively apply this bipartitioning method to the component
subgraphs to obtain $k$-partitions, but there is a more elegant
approach involving additional eigenvectors that we shall discuss next.

\section{Spectral k-partitioning}


%#######################################################################
