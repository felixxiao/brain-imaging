In the previous chapter, we showed that the problem of finding the
minimum ratio cut of a graph (with Laplacian matrix $L$, degree matrix
$D$, and adjacency matrix $A$) can be formulated as minimizing
\begin{equation} \label{ratio_cut}
\Tr(R^T L R)
\end{equation}
over the set, $\mathcal{R}$, of $n \times k$ matrices satisfying
\begin{enumerate}
\item
$R^T R = I$

\item
$R \geq 0$ (element-wise)

\item
$R R^T u_n = u_n$ where $u_n$ is a $n$-dimensional vector of all ones.
\end{enumerate}

If the sizes of the components in the optimal ratio cut partition
are perfectly balanced, which is equivalent to saying if the diagonal
of the optimal ratioed assignment matrix $R R^T$ has entries all
equal to $\frac{k}{n}$, then
\begin{align*}
\Tr(R^T D R) &= \sum_{i=1}^n [R R^T]_{ii} D_{ii} \\
             &= \sum_{i=1}^n \frac{k}{n} D_{ii} \\
             &= \frac{k}{n} \sum_{i,j} A_{ij}
\end{align*}
is a constant that does not depend on $R$. The same is true if each
vertex has the same degree $D_{ii} = d$, in which case
\begin{align*}
\Tr(R^T D R) &= \sum_{i=1}^n [R R^T]_{ii} D_{ii} \\
             &= d \sum_{i=1}^n [R R^T]_{ii} \\ 
             &= d k
\end{align*}
is also a constant that does not depend on $R$. In either case,
\[ \argmin{R \in \mathcal{R}} \Tr(R^T L R)
 = \argmax{R \in \mathcal{R}} \Tr(R^T A R) \]
This equality may also hold even if neither condition is true,
especially if they are approximately true.

Spectral $k$-partitioning drops the second and third constraints
of $\mathcal{R}$ to derive a closed-form minimizer of \ref{ratio_cut},
from which the original assignment matrix can be obtained by $k$-means.
This chapter deals with an alternative relaxation of $\mathcal{R}$ that
drops the first and third constraints.

\section{Nonnegative Matrix Factorization}

For an $n \times m$ matrix $A$, a nonnegative matrix factorization
(NMF) is a pair of matrices $W \in \R^{n \times k}$ and
$H \in \R^{m \times k}$ that minimizes $\| A - W H^T \|_F^2$
subject to elementwise nonnegativity: $H \geq 0$ and $W \geq 0$.
Here, $\|X\|_F = \sqrt{\sum_{ij} X_{ij}}$ refers to the Frobenius norm.

For $n \times n$ symmetric matrices $A$, a \textit{symmetric} NMF
(SymNMF) is a matrix $H \in \R^{n \times k}$ that minimizes
$\| A - H H^T \|_F^2$, and $k$ is an arbitrary positive integer
typically much smaller than $n$.

The following theorem from \cite{Ding:05} illustrates the connection
between SymNMF and graph partitioning.

\begin{theorem}
Let $A$ be a $n \times n$ symmetric matrix. Then
\[ \argmax{H^T H = I, H \geq 0} \Tr(H^T A H)
 = \argmin{H^T H = I, H \geq 0} \| A - H H^T \|_F^2 \]

Proof. \begin{align*}
   \argmax{H^T H = I, H \geq 0} \Tr(H^T A H)
&= \argmin{H^T H = I, H \geq 0} -2 \Tr(H^T A H) \\
&= \argmin{H^T H = I, H \geq 0} \Tr(A A^T) - 2 \Tr(H^T A H)
                                + \|H^T H\|_F^2 \\
&= \argmin{H^T H = I, H \geq 0} \|A - H H^T\|_F^2
\end{align*}
\end{theorem}

If $A$ is the adjacency matrix, then under the equal vertex degrees
condition described earlier
$ \argmax{H^T H = I, H \geq 0} \Tr(H^T A H)
= \argmin{H^T H = I, H \geq 0} \Tr(H^T L H)$.
Hence an alternative approach to the minimum ratio-cut problem
is to drop the $H^T H = I$ constraint and solve the SymNMF problem:

\begin{equation} \label{sym_nmf}
\begin{aligned}
\min_{H \in \R^{n \times k}} &\;& \|A - H H^T\|_F^2 \\
\text{s.t.}                  &\;& H \geq 0          \\
\end{aligned}
\end{equation}

This relaxation has two key differences from the spectral relaxation
\label{spectral_k-partition}.
\begin{itemize}
\item
There is no closed-form solution, and the optimal value is found
via an optimization algorithm, described in the next section.

\item
The optimal assignments are recovered directly from the largest
entry in each row. There is no need for $k$-means.
\end{itemize}

\section{An Alternating Nonnegative Least Squares Algorithm for SymNMF}

\cite{Kuang:15} re-formulates \ref{sym_nmf} as a non-symmetric NMF
with a penalty on the difference between the two matrix factors:
\begin{equation} \label{nonsym_nmf}
\min_{W,H \geq 0} \|A - W H^T\|_F^2 + \alpha \|W - H\|_F^2
\end{equation}
where $W,H \in \R^{n \times k}$. The $\alpha$ parameter 

The rationale for this to use known methods for solving the
non-symmetric NMF and adapt them to the symmetric problem.
One powerful framework for solving NMF is Alternating Nonnegative
Least Squares (ANLS), which factors $A$ into nonnegative $W$ and $H$
by fixing the $H$ matrix and solving for $W$:
$$ W \gets \argmin{W \geq 0} \|A - W H^T\|_F^2 $$
and fixing this new matrix $W$ and solving for $H$:
$$ H \gets \argmin{H \geq 0} \|A - W H^T\|_F^2 $$
and repeating the two steps until convergence.
Both subproblems in the ANLS framework are convex, and the algorithm
requires only an initial $W$ to get started.

\cite{Kuang:15} describes an algorithm for solving SymNMF that uses
the ANLS framework. The objective function in \ref{nonsym_nmf} can be
re-written as
\begin{equation} \label{nls}
\norm{ \begin{bmatrix} W \\ \sqrt{\alpha} I_k \end{bmatrix} H^T
     - \begin{bmatrix} A \\ \sqrt{\alpha} W^T \end{bmatrix} }_F^2
\end{equation}
with $\begin{bmatrix} W \\ \sqrt{\alpha} I_k \end{bmatrix}$ taking on
the part of the fixed matrix and $H$ the decision matrix. The ANLS
algorithm for SymNMF is the following:

\begin{algorithm}
\caption{ANLS algorithm for SymNMF}
\begin{algorithmic}[1]
\State Initialize $H$
\Repeat
  \State $W \gets H$
  \State $H \gets \argmin{H \geq 0}
    \norm{ \begin{bmatrix} W \\ \sqrt{\alpha} I_k \end{bmatrix} H^T
         - \begin{bmatrix} A \\ \sqrt{\alpha} W^T \end{bmatrix} }_F^2$
\Until{convergence}
\end{algorithmic}  
\end{algorithm}

The $\alpha$ can be increased each iteration to force convergence of
$W$ and $H$. A recommended strategy is to multiply $alpha$ by 1.01 each
iteration.

\subsection{Block Principal Pivoting for Nonnegative Least Squares}
For details on this method, please refer to \cite{Kim:11}.

\section{0-1 Symmetric Matrix Factorization}

\subsection{Stochastic Gradient-Like Method}

\subsection{Mixed Integer Programming Method}
