In the previous chapter, we showed that the problem of finding the
minimum ratio cut of a graph (with Laplacian matrix $L$, degree matrix
$D$, and adjacency matrix $A$) can be formulated as minimizing
\begin{equation} \label{ratio_cut}
\Tr(R^T L R)
\end{equation}
over the set, $\mathcal{R}$, of $n \times k$ matrices satisfying
\begin{enumerate}
\item
$R^T R = I$

\item
$R \geq 0$ (element-wise)

\item
$R R^T u_n = u_n$ where $u_n$ is a $n$-dimensional vector of all ones.
\end{enumerate}

If the sizes of the components in the optimal ratio cut partition
are perfectly balanced, which is equivalent to saying if the diagonal
of the optimal ratioed assignment matrix $R R^T$ has entries all
equal to $\frac{k}{n}$, then
\begin{align*}
\Tr(R^T D R) &= \sum_{i=1}^n [R R^T]_{ii} D_{ii} \\
             &= \sum_{i=1}^n \frac{k}{n} D_{ii} \\
             &= \frac{k}{n} \sum_{i,j} A_{ij}
\end{align*}
is a constant that does not depend on $R$. The same is true if each
vertex has the same degree $D_{ii} = d$, in which case
\begin{align*}
\Tr(R^T D R) &= \sum_{i=1}^n [R R^T]_{ii} D_{ii} \\
             &= d \sum_{i=1}^n [R R^T]_{ii} \\ 
             &= d k
\end{align*}
is also a constant that does not depend on $R$. In either case,
\[ \argmin{R \in \mathcal{R}} \Tr(R^T L R)
 = \argmax{R \in \mathcal{R}} \Tr(R^T A R) \]
This equality may also hold even if neither condition is true,
especially if they are approximately true.

Spectral $k$-partitioning drops the second and third constraints
of $\mathcal{R}$ to derive a closed-form minimizer of \ref{ratio_cut},
from which the original assignment matrix can be obtained by $k$-means.
This chapter deals with an alternative relaxation of $\mathcal{R}$ that
drops the first and third constraints.

\section{Symmetric Nonnegative Matrix Factorization}

For an $n \times m$ matrix $A$, a nonnegative matrix factorization
(NMF) is a pair of matrices $W \in \R^{n \times k}$ and
$H \in \R^{m \times k}$ that minimizes $\| A - W H^T \|_F^2$
subject to elementwise nonnegativity: $H \geq 0$ and $W \geq 0$.
Here, $\|X\|_F = \sqrt{\sum_{ij} X_{ij}}$ refers to the Frobenius norm.

For $n \times n$ symmetric matrices $A$, a \textit{symmetric} NMF
(SymNMF) is a matrix $H \in \R^{n \times k}$ that minimizes
$\| A - H H^T \|_F^2$, and $k$ is an arbitrary positive integer
typically much smaller than $n$.

The following theorem from \cite{Ding:05} illustrates the connection
between SymNMF and graph partitioning.

\begin{theorem}
Let $A$ be a $n \times n$ symmetric matrix. Then
\[ \argmax{H^T H = I, H \geq 0} \Tr(H^T A H)
 = \argmin{H^T H = I, H \geq 0} \| A - H H^T \|_F^2 \]

Proof. \begin{align*}
   \argmax{H^T H = I, H \geq 0} \Tr(H^T A H)
&= \argmin{H^T H = I, H \geq 0} -2 \Tr(H^T A H) \\
&= \argmin{H^T H = I, H \geq 0} \Tr(A A^T) - 2 \Tr(H^T A H)
                                + \|H^T H\|_F^2 \\
&= \argmin{H^T H = I, H \geq 0} \|A - H H^T\|_F^2
\end{align*}
\end{theorem}

If $A$ is the adjacency matrix, then under the equal vertex degrees
condition described earlier
$ \argmax{H^T H = I, H \geq 0} \Tr(H^T A H)
= \argmin{H^T H = I, H \geq 0} \Tr(H^T L H)$.
Hence an alternative approach to the minimum ratio-cut problem
is to drop the $H^T H = I$ constraint and solve the SymNMF problem:

\begin{equation} \label{sym_nmf}
\begin{aligned}
\min_{H \in \R^{n \times k}} &\;& \|A - H H^T\|_F^2 \\
\text{s.t.}                  &\;& H \geq 0          \\
\end{aligned}
\end{equation}

This relaxation has two key differences from the spectral relaxation
\label{spectral_k-partition}.
\begin{itemize}
\item
There is no closed-form solution, and the optimal value is found
via an optimization algorithm, described in the next section.

\item
The optimal assignments are recovered directly from the largest
entry in each row. There is no need for $k$-means.
\end{itemize}

\subsection{The Alternating Nonnegative Least Squares Algorithm}

The algorithm for solving (\ref{sym_nmf}) developed by \cite{Kuang:15}
uses the same framework as existing algorithms for solving the
asymmetric NMF problem: $\min_{W,H \geq 0} \|A - W H^T\|_F^2$.
That framework, called \textit{alternating nonnegative least squares}
is an iterative scheme that fixes one of the matrix factors and
solves for the other. Then, it fixes the factor just solved and
solves for the first. In other words, these two steps are repeated
until convergence:
\begin{enumerate}
  \item $ W \gets \argmin{W \geq 0} \|A - W H^T\|_F^2 $
  \item $ H \gets \argmin{H \geq 0} \|A - W H^T\|_F^2 $
\end{enumerate}
One reason this method is effective in the asymmetric case is
that the two subproblems are convex, so it is easy to attain the
global minimum in each subproblem. Further, a number of specialized
algorithms have been developed to solve these nonnegative least
squares problems quickly. One in particular by \cite{Kim:11} works
well with large, sparse $A$ matrices and is detailed in the next
subsection.

An approach for SymNMF in \cite{Kuang:15} uses this framework by
artificially creating two different factors rather than one
and adding on an adjustable penalty term for the difference
between the two factors:
\begin{equation} \label{nonsym_nmf}
\min_{W,H \geq 0} \|A - W H^T\|_F^2 + \alpha \|W - H\|_F^2
\end{equation}
where $W,H \in \R^{n \times k}$. As in the asymmetric case, the
algorithm computes $W$ and $H$ iteratively by fixing one of the
two factors. The difference is the introduction of the penalty
term, $\alpha$, which can be increased after each iteration to
force the eventual convergence of $W$ and $H$.

(\ref{nonsym_nmf}) can also be re-written in the same form as the
asymmetric NMF:
\begin{equation} \label{nls}
\norm{ \begin{bmatrix} W \\ \sqrt{\alpha} I_k \end{bmatrix} H^T
     - \begin{bmatrix} A \\ \sqrt{\alpha} W^T \end{bmatrix} }_F^2
\end{equation}
with $\begin{bmatrix} W \\ \sqrt{\alpha} I_k \end{bmatrix}$ taking on
the part of the fixed matrix and $H$ the decision matrix.
The ANLS algorithm for SymNMF is the following:

\begin{algorithm}
\caption{ANLS algorithm for SymNMF}
\label{anls_symnmf}
\begin{algorithmic}[1]
\State Initialize $H$
\Repeat
  \State $W \gets H$
  \State $H \gets \argmin{H \geq 0}
    \norm{ \begin{bmatrix} W \\ \sqrt{\alpha} I_k \end{bmatrix} H^T
         - \begin{bmatrix} A \\ \sqrt{\alpha} W^T \end{bmatrix} }_F^2$
  \State increase $\alpha$
\Until{convergence}
\end{algorithmic}
\end{algorithm}

\cite{Kuang:15} recommends multiplying $alpha$ by 1.01 each iteration.
The next section describes the method in \cite{Kim:11} for solving the
embedded nonnegative least squares problem.

\subsection{Block Pivoting Algorithm for Nonnegative Least Squares}
Let us consider a simplified form of the NLS step in Algorithm
\ref{anls_symnmf}. Let $x$ denote a single $k$-dimensional row of $H$,
and our goal is to solve
\begin{equation} \label{single_column_nls}
\min_{x \geq 0} \| C x - b \|_2^2
\end{equation}
where $C = \begin{bmatrix} W \\ \sqrt{\alpha} I_k \end{bmatrix}$ and
$b$ is the column of
$\begin{bmatrix} A \\ \sqrt{\alpha} W^T \end{bmatrix}$ with the same
index as the index of row $x$ in $H$. Since the rows of $H$ share no
constraints with one another, the general NLS problem in Algorithm
\ref{anls_symnmf} can be viewed simply as $n$ independent cases of
\ref{single_column_nls}.

The Karush-Kuhn-Tucker necessary conditions for optimality are:
\begin{enumerate}
\item
$y = C^T C x - C^T b$
\item
$y \geq 0$
\item
$x \geq 0$
\item
$x_i y_i = 0$ for $i = 1, ..., k$
\end{enumerate}
Since $I_k$ is full column rank, so is $C$, making $C^T C$ positive
definite and \ref{single_column_nls} strictly convex.

Let $x^*$ be the optimal solution to \ref{single_column_nls}. Suppose
the indices of the strictly positive entries of $x^*$ were known and
denote them with $F$. Then all the entries of $x^*$ and $y^*$ can be
computed. Let $x_F$ denote the vector of entries of $x$ indexed by $F$
and $C_F$ is the matrix of \textit{columns} of $C$ indexed by $F$. Let
$G$ be all the elements of $\{1, ..., k\}$ not in $F$. Then,
\begin{subequations} 
\begin{equation} \label{x_F}
x_F = \argmin{x_F \geq 0} \|C_F x_F - b\|_2^2
\end{equation}
\begin{equation} \label{x_G}
x_G = 0
\end{equation}
\begin{equation} \label{y_F}
y_F = 0
\end{equation}
\begin{equation} \label{y_G}
y_G = C_G^T (C_F x_F - b)
\end{equation}
\end{subequations}
(\ref{x_F}) is implied by the fact that the strictly positive entries
of $x$ must solve the unconstrained problem. (\ref{x_G}) is true by
definition of $F$ and $G$. (\ref{y_F}) is implied by KKT condition 4.
(\ref{y_G}) is implied by KKT condition 1. If index sets $F$ and $G$
give the optimal solution, then $x_F$ and $y_G$ must satisfy KKT
conditions 2 and 3 (nonnegativity). Therefore the
\textit{Block Pivoting Algorithm} finds the optimal $x$ and $y$ by
searching for the correct $F$ and $G$ index sets.

Suppose the $x$ and $y$ computed from the current $F$ and $G$ do not
satisfy the KKT conditions, or equivalently the set
\begin{equation} \label{infeasible}
V = \{j \in F : x_j < 0\} \cup \{j \in G : y_j < 0 \}
\end{equation}
is not empty. The algorithm updates $F$ and $G$ by taking a subset
$\hat{V} \subseteq V$ and setting
\begin{subequations} \label{update_F_G}
\begin{equation}
F \gets (F - \hat{V}) \cup (\hat{V} \cap G)
\end{equation}
\begin{equation}
G \gets (G - \hat{V}) \cup (\hat{V} \cap F)
\end{equation}
\end{subequations}
By default, the algorithm uses $\hat{V} = V$ for speed. However, there
are cases where this could result in an infinite loop, so the single
pivot rule $\hat{V} = \max \{j \in V\}$ is invoked instead, despite
being slower.

In the multicolumn case 
\begin{equation} \label{multi_column_nls}
\min_{X \geq 0} \| C X - B \|_2^2
\end{equation}
$X$ is $k \times n$ and the algorithm needs to track, for each column of
$X$, the $F$ and $G$ indices. Since in many cases including ours,
$n \gg k$, so many columns of $X$ are likely to have the same $F$ and $G$.
Given this fact, the block pivoting algorithm has a shortcut to solving
(\ref{x_F}) and (\ref{y_G}) for each column. (\ref{x_F}) is found by solving
$C_F^T C_F x_F = C_F^T b$ and the Cholesky factorization for this can be done
just once for all columns with the same $F$ indices. Additionally, the
$C_F^T C_F$, $C_F^T b$, $C_G^T C_F$, and $C_G^T b$ matrices and vectors
required to compute $x_F$ and $y_G$ can be extracted as submatrices of $C^T C$
and $C^T B$, computed once in the beginning of the algorithm. In our SymNMF
case, these two matrices are much smaller than the original sparse $A$ matrix
that formed them.
\begin{equation} \label{CTC}
C^T C = W^T W + \alpha I_k
\end{equation}
\begin{equation} \label{CTB}
C^T B = W^T A + \alpha W^T
\end{equation}

\begin{algorithm}
\caption{Block Pivoting Algorithm for NLS}
\label{nls_alg}
\begin{algorithmic}
\State \textbf{Input:} $A \in \R^{n \times n}$, $W \in \R^{n \times k}$
\State \textbf{Output:} $X$
\State Compute $C^T C$ and $C^T B$ by (\ref{CTC}) and (\ref{CTB})
\State Initialize $F_i = \emptyset$ and $G_i = \{1, ..., k\}$
       for all $i = 1, ..., n$
\State Initialize $\alpha (\in \R^n) = 3$ and $\beta (\in \R^n) = k + 1$
\Repeat
  \State Update $X$ and $Y$ according to $F$ and $G$ by column-grouping.
  \State $I \gets \{i : (X_{F_j}, Y_{G_j}) \text{ is infeasible}\}$
  \ForAll{$i \in I$}
    \State Compute $V_i$ by (\ref{infeasible})
    \If{$|V_i| < \beta_i$}
      \State $\beta_i \gets |V_i|$
      \State $\alpha_i \gets 3$
      \State $\hat{V}_i \gets V_i$
    \ElsIf{$\alpha_i \geq 1$}
      \State $\alpha_i \gets \alpha_i - 1$
      \State $\hat{V}_i \gets V_i$
    \ElsIf{$\alpha_i = 0$}
      \State $\hat{V}_i = \max \{j \in V_i\}$
    \EndIf
    \State Update $F_i$ and $G_i$ by (\ref{update_F_G})
  \EndFor
\Until{$I = \emptyset$}
\end{algorithmic}
\end{algorithm}

\section{Connected Partitions}

An issue with the SymNMF approach is that the partitions may not be
connected -- there may exist a vertex, none of whose neighbor vertices
belong to the same parcel that it does.

\begin{definition}[Unweighted Adjacency Matrix]
For a graph with $n$ vertices and edge set $E$, the unweighted adjacency
matrix $B \in \{0, 1\}^{n \times n}$ has entries
\[ B_{ij} = \begin{cases}
  1 & \text{if } (i,j) \in E \\
  0 & \text{otherwise}
\end{cases}\]
\end{definition}

\begin{prop} \label{connected}
Let $B$ be the unweighted adjacency matrix of a graph with $n$ vertices
and $X \in \{0, 1\}^{n \times K}$ be an assignment matrix satisfying
$X e_K = e_n$ and $X^T e_n \geq e_K$. If the partitions of the graph
defined by $X$ are connected then
\[ (B - I) X \geq 0 \]

\textit{Proof:} Let $b_i$ denote the $i$th row of $B$ and $X_k$ the
$k$th column of $X$. Since the non-zero entries of $b_i$ indicate
which vertices $i$ is neighboring and the non-zero entries of $X_k$
indicate which vertices are in the $k$th partition, the dot product
$b_i X_k$ equals the number of vertices neighboring $i$ that are in the
$k$th partition.

If the partitions of the graph are connected, then this number is
at least 1 if $k$ is the partition containing $i$. If $k$ does not
contain $i$, then $b_i X_k$ can be 0. This can be succinctly expressed
as $b_i X_k \geq X_{ik}$ for all $i = 1, ..., n$ and $k = 1, ..., K$,
which is equivalent to the matrix inequality above.
\end{prop}

This fact will be used in the next section for the problem of finding
a binary matrix factorization of $A$.


\section{Symmetric 0-1 Matrix Factorization}

The SymNMF method of graph partitioning finds continuous nonnegative
matrix $H \in R^{n \times k}$ so as to minimize $\norm{A - H H^T}_F^2$
and obtains the 0-1 assignment matrix $X$ by thresholding $H$.
In the graph partitioning case, it arguably more desirable to obtain
binary $X$ directly rather than via the continuous $H$.

This leads us to the \textit{Symmetric Binary Matrix Factorization}
problem, henceforth called SymBMF. Formally, for a symmetric matrix
$A \in [0, 1]^{n \times n}$ we want to solve
\begin{center}
\begin{tabular}{l l}
minimize   & $\norm{A - X X^T}_F^2$ \\
subject to & $X \in \{0, 1\}^{n \times k}$ \\
           & $X e_k = e_n$
\end{tabular}
\end{center}
The constraint $X^T e_n \geq 1$ can be added to ensure no column of $X$
contains all zeros. Additionally, the constraint $(B - I) X \geq 0$
(where $B$ is the unweighted adjacency matrix) can be added for graph
partitioning purposes to ensure all the partitions are connected
(\ref{connected}).

It is desirable for algorithms that solve SymBMF to work well when $A$
is sparse or incomplete. In the sparse case, when most entries of $A$
are zero, the complexity of the algorithm ideally ought to scale with
the number of non-zero entries, and not with the number of rows and
columns. Similarly in the case of incomplete $A$ there are entries of
$A$ that are unknown or that we simply don't care about approximating.
This changes the objective function from a matrix norm to a summation:
\[ \sum_{(i,j) \in A} (A_{ij} - x_i^T x_j)^2 \]
where $x_i$ denotes the $i$th row of $X$.

We present two methods that both scale well in sparse $A$ and can
handle incomplete $A$. The first is a very fast local minimizer
inspired by methods from multidimensional scaling. The second is a
mixed integer program that solves the problem globally, but is not
as efficient.

\subsection{An MDS-Inspired Method}

The central idea behind this to begin with an random $n \times k$
binary matrix $X$ that satisfies $X e_k = e_n$ and iteratively
edit each row of $X$ so as to minimize $\|A - X X^T\|_F^2$ locally.
Editing a row of $X$ here means determining which column to place the 1
in. The change made in row $i$ of $X$ only impacts the $i$th row and
$i$th column of $A - X X^T$. The column to place the 1 in that minimizes
the objective locally is:
\begin{equation} \label{col_select}
k^* = \argmin{k = 1,...,K} \|A_i - X_k\|^2
\end{equation}
where $A_i$ refers to the $i$th column of $A$ and $X_k$ to the $k$th
column of $X$. This is because the $i$th column of $X X^T$ is $X_k$,
where $k$ is the parcel that vertex $i$ has been assigned to.

In the case of incomplete matrix $A$, the column selection rule in
\ref{col_select} should replaced by
\begin{equation} \label{inc_col_select}
k^* = \argmin{k = 1,...,K} \sum_{j \in A_i} (A_{ij} - X_{jk})^2
\end{equation}
The squared terms in (\ref{col_select}) and (\ref{inc_col_select}) can
also be changed to absolute value.

\begin{algorithm}
\caption{SymBMF}
\begin{algorithmic}[1]
\State Initialize $X \in \{0, 1\}^{n \times k}$ such that $X e_k = e_n$
\State $i \gets 1$
\State $j \gets 1$
\Repeat
  \State $k \gets$ index of 1 entry of $x_i$
  \State $x_{ik} \gets 0$
  \State Compute $k^*$ by (\ref{col_select}) or (\ref{inc_col_select})
  \State $x_{ik^*} \gets 1$
  \If{$k \neq k^*$} \Comment{row $i$ has been edited}
    \State $j \gets i$
  \EndIf
  \State $i \gets (i \mod n) + 1$
\Until{$i = j$}
\end{algorithmic}
\end{algorithm}

The stopping criterion halts the loop before iteration $i$ if no rows
have been edited since $x_i$ was last edited.



\subsection{Mixed Integer Programming Method}

The MIP method begins by replacing objective's squared term in the
Frobenius norm with an L1 penalty. This new objective function is
equivalent to the original if $A$ is binary.
\begin{center}
\begin{tabular}{l l l}
minimize   & $\sum_{(i,j) \in A} |A_{ij} - x_i^T x_j|$ \\
subject to & $x_i \in \{0, 1\}^k$ & for $i = 1, ..., n$ \\
           & $e_k^T x_i = 1$ & for $i = 1, ..., n$
\end{tabular}
\end{center}
This problem would be a mixed integer program if it were not for the
quadratic $x_i^T x_j$ in the objective. We substitute a new variable
$y_{ij} = x_i^T x_j$ and find linear constraints that make this
relation true for binary $x_i$ satisfying $e_k^T x_i = 1$.
Note that an equivalent definition of
$y_{ij}$ is
\[ y_{ij} = \begin{cases}
  1 & \text{if } x_i = x_j \\
  0 & \text{otherwise}
\end{cases}\]
The following lemma provides an linearization of $x_i^T x_j$:

\begin{lemma}
Let $x_i$ and $x_j$ both be $k$-dimensional binary vectors that sum to 1.
Then $y_{ij} = x_i^T x_j$ is equivalent to
\[ y_{ij} \leq \min (x_i - x_j) + 1 \]
\[ y_{ij} \geq \max (x_i + x_j) - 1 \]

\textit{Proof:} Follows from the fact that $\min (x_i - x_j) + 1$ and
$\max (x_i + x_j) - 1$ both equal 1 if $x_i$ and $x_j$ are equal
and 0 if not.
\end{lemma}

Substituting $y_{ij}$ and adding the above linear constraints gives us
the following MIP equivalent of SymBMF.

\begin{center}
\begin{tabular}{l l l}
minimize   & $\sum_{(i,j) \in A} |A_{ij} - y_{ij}|$ \\
subject to & $x_i \in \{0, 1\}^k$ & for $i = 1, ..., n$ \\
           & $e_k^T x_i = 1$ & for $i = 1, ..., n$ \\
           & $\begin{cases}
             y_{ij} \leq x_{ik} - x_{jk} + 1 \\
             y_{ij} \geq x_{ik} + x_{jk} - 1
             \end{cases}$
           & for $(i,j) \in A$, $k = 1, ..., K$
\end{tabular}
\end{center}

To enforce partition connectedness we can add the constraint
$(B - I) X \geq 0$ from (\ref{connected}) where $B$ is the unweighted
assignment matrix.
