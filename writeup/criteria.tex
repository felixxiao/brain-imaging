% 1. in between-score and boundary-score, split summation indexation
%    into 2 lines
%#######################################################################

In chapter one we discussed our graphical approach to the brain
parcellation problem. We construct a weighted undirected graph where
each vertex is a voxel. The graph reflects the spatial position of the
voxels; it connects each vertex to the vertices representing the voxel's
six cubically adjacent neighbors. The weights on these edges are sample
distance correlation statistics between the adjacent voxels in
the time series and they measure statistical dependence, with 0 denoting full independence and 1 full dependence. Let $G(V, E)$ denote
this graph, its vertices, and its edges.

In this context, a valid $k$-fold partition $\mathcal{P}_k$ of the
graph $G$ is a collection of vertex subsets $(V_1, ..., V_k)$
satisfying the following:

\begin{enumerate}[1.]
\item
$V_i \neq \emptyset$ for all $V_i \in \mathcal{P}_k$

\item
$\bigcup\limits_{i=1}^k V_i = V$

\item
$V_i \cap V_j = \emptyset$ for all $V_i, V_j \in \mathcal{P}_k$

\item
$V_i$ is connected (i.e. for every two vertices in $V_i$, there is a
path between them) for all $V_i \in \mathcal{P}_k$
\end{enumerate}

In this chapter we will suggest various criteria for measuring the
goodness-of-fit of partitions and discuss their statistical and
computational advantages and drawbacks.


\section{Within-Parcel Similarity}

Voxels in the same parcel are ideally highly dependent on one another in
the time series of fMRI data. As discussed in the previous chapter,
distance correlation is a good measure of dependence. The distance
correlation between two random vectors equals zero if and only if the
two random vectors are independent, which is not true of correlation
statistics such as Pearson's.

Let $\mathcal{R}(x,y)$ denote the distance correlation between two
voxels $x$ and $y$.
Let $V$ and $W$ be any parcels. We will use $E_V$ to denote the set of edges with one end in $V$ and one end not in $V$, and $E_{V,W}$ the set
of edges with one end in $V$ and one in $W$. {\color{red}Mention if we want to
maximize or minimize these criteria. Do it for each of them}

\begin{definition}[Within-Score] \label{within-score}
\[ \frac{1}{k} \sum_{V \in \mathcal{P}_k}
   \frac{1}{|V|^2} \sum_{x,y \in V} \mathcal{R}(x,y)
\]
\end{definition}

The Within-Score is non-spatial; it considers all pairs of voxels
equally regardless of whether they are adjacent. As a result, it is a
good measure of how much the voxels within each parcel are dependent on
each other as a set. The downside of this criterion is that it is very
expensive to compute. With over 300,000 voxels in an fMRI data set we
would potentially have to compute tens of billions of distance
correlation statistics, each of which takes time proportional to the
number of samples squared.

An alternative and far less expensive criterion that measures within-
parcel similarity works by counting distance correlations between
adjacent pairs of voxels.

\begin{definition}[Adjacent-Score] \label{adjacent-score}
\[ \frac{1}{k} \sum_{V \in \mathcal{P}_k}
   \frac{1}{|E_{V,V}|} \sum_{(x,y) \in E_{V,V}} \mathcal{R}(x,y)
\]
\end{definition}

Rather than treat parcels as sets with no spatial information, the
Adjacent-Score does the opposite by only considering the pairwise
dependency of adjacent voxels. For sparse graphs such as ours, the number
of distance correlation computations is proportional to the number of
vertices.

An intermediate possibility we did not explore is to consider all pairs
of voxels up to some maximum spatial distance from each other and
perform a weighted averaging of sample pairwise distance correlations,
with weights that depend on spatial distance.

\section{Between-Parcel Dissimilarity}

To evaluate parcellation quality, it is also useful to measure how
dependent voxels belonging to different parcels are on each other. To
this end we define two criterion similar to the Within-Parcel criterion;
a non-spatial metric called the Between-Score and its spatial metric
the Boundary-Score.

\begin{definition}[Between-Score] \label{between-score}
\[ \frac{1}{\binom{k}{2}} \sum_{V, W \in \mathcal{P}_k, V \neq W}
   \frac{1}{|V||W|} \sum_{x \in V, y \in W} \mathcal{R}(x,y)
\]
\end{definition}

\begin{definition}[Boundary-Score] \label{boundary-score}
\[ \frac{1}{\binom{k}{2}} \sum_{V, W \in \mathcal{P}_k, V \neq W}
   \frac{1}{|V||W|} \sum_{(x,y) \in E_{V,W}} \mathcal{R}(x,y)
\]
\end{definition}

Generally both of these quantities are more expensive to compute than
their Within-Parcel counterparts. Boundary-Score is easy enough to
compute for validation purposes, but does not convey much additional
information beyond what the Adjacency-Score does, in the sense that
the edges used in the computation of Adjacency-Score are the complement
of the edges used in the Boundary-Score.

The ability of distance correlation to generalize to pairs of random
vectors of arbitrary dimension gives us another way of computing
the dependency between two parcels. The Multivariate Between-Score
defined below treats parcels as random vectors and computes the distance
correlation at the parcel level rather than voxel level. The result is
a measure of non-spatial between-parcel similarity that is also
computationally feasible. For this reason we will use Multivariate
Between-Score as our primary measure of parcel dissimilarity.

\begin{definition}[Multivariate Between-Score]
\label{multi-between-score}
\[ \frac{1}{\binom{k}{2}} \sum_{V, W \in \mathcal{P}_k, V \neq W}
   \mathcal{R}(V, W)
\]
\end{definition}


\section{Graph Cuts}

Closely related to the Boundary-Score is the notion of a graph cut from
computer science. A \textit{cut set} is the set of edges with endpoints
in different parcels. The \textit{cut weight} is the sum of weights of
all edges in the cut set and can be expressed as
\[ \frac{1}{2} \sum_{V \in \mathcal{P}_k}
   \sum_{x,y \in E_V} \mathcal{R}(x,y) \]
The \textit{ratio cut} defined below is a weighted version of the cut
weight:
\[ \frac{1}{2} \sum_{V \in \mathcal{P}_k} \frac{1}{|V|}
   \sum_{x,y \in E_V} \mathcal{R}(x,y)
\]
The subfield of graph partitioning is concerned with minimizing
cut weight, ratio cut, and several other related quantities. Over the
last several decades a number of highly effective approximation
algorithms have been developed to find partitions of graphs that
minimize these quantities. Later chapters will explore how these methods
work for brain parcellation.

\section{Balance and Jaggedness}

In addition to the above distance correlation based criteria, there are
two additional metrics concerned with parcel shape.

\begin{definition}[Balance] \label{balance}
\[ \frac{1}{k} \frac{1}{\max_{V \in \mathcal{P}_k} |V|}
   \sum_{V \in \mathcal{P}_k} |V| \]
\end{definition}

The Balance-Score ranges from 1 (all equally sized parcels) to
0 (two parcels with one of size zero).

\begin{definition}[Jaggedness] \label{jaggedness}
\[ \frac{1}{k} \sum_{V \in \mathcal{P}_k} \frac{|E_V|^\frac{3}{2}}{|V|}
\]
\end{definition}

The $\frac{3}{2}$ power makes the ratio invariant on parcel size. 
{\color{red}Not a fan of this sentence about ratio invariant. Maybe just
say it's used to account for the fact that numerator is a surface area-like
measurement while the denominator is a volume-like measurement.}
For instance, a $n \times n \times n$ cube of vertices would have
a jaggedness of $6^{\frac{3}{2}}$ which does not depend on $n$.

\section{Comparing Multiple Parcellations}

{\color{red}Put the stuff about rand index here}
A common criterion for measuring the similarity of two partitionings
is Rand Index, defined as
